[
{
	"uri": "http://localhost:1313/ai/getting-connected/",
	"title": "Getting Connected",
	"tags": [],
	"description": "",
	"content": "For the purposes of this training session, we have provisioned a single OpenShift cluster, with OpenShift AI deployed on it.\nEach person attending this lab will have a unique user account in which to do their work.\nEnvironment information In a new window or tab, open the following URL and log in:\nThe Red Hat OpenShift AI Dashboard URL for our shared environment: https://rhods-dashboard-redhat-ods-applications.apps.riviera-dev-2024.sandbox2830.opentlc.com Enter your credentials (distributed on a paper during the lab) The result should look like: Because the password is so simple, your browser might display a scary message such as: It is safe here to ignore this message when it pops up.\nAfter you authenticate, the result should look like: If you got this far and saw all that, congratulations, you properly connected to the OpenShift AI Dashboard Application!\nWe are now ready to start the lab.\n"
},
{
	"uri": "http://localhost:1313/overview/goal/",
	"title": "Objectif",
	"tags": [],
	"description": "",
	"content": "Your goal will be to TODO\n"
},
{
	"uri": "http://localhost:1313/overview/",
	"title": "Presentation",
	"tags": [],
	"description": "",
	"content": "Part 1 Presentation Welcome to this lab where TODO\n"
},
{
	"uri": "http://localhost:1313/overview/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Architecture TODO\n"
},
{
	"uri": "http://localhost:1313/ai/",
	"title": "Artificial Intelligence (1h)",
	"tags": [],
	"description": "",
	"content": "Part 2 Artificial Intelligence "
},
{
	"uri": "http://localhost:1313/ai/creating-project/",
	"title": "Connecting to your project and pipeline server",
	"tags": [],
	"description": "",
	"content": "As a preliminary step, each of you is going to\nConnect to a Data Science project\nthis will help keep your things together Create a Data Connection\nwe need that for the pipeline server to store its artifacts Deploy a Data Science Pipeline Server\nwe will need one, and it\u0026rsquo;s better to create it from the start Launch a Workbench\nwe will use it to review content and notebooks Clone the git repo into your Workbench\nthis contains all the code from the prototype The instructions below will guide you through these steps. Follow them carefully.\nConnect a project First, in the OpenShift AI Dashboard application, navigate to the Data Science Projects menu on the left: A project with the same name as your user id has been created for you. You have been assigned a unique user ID at the begining of the lab. Remember this user ID for the instances creation. Click on the available project project. You should land on a similar page: Create a Data Connection for the pipeline server We have deployed an instance of Minio in the cluster to act as a simple Object Storage for our purposes.\nYou will need to Add data connection that points to it. Scroll down to the bottom of data science project page and click on \u0026ldquo;Data Connections\u0026rdquo;: You will land to and empty state page. Click on \u0026ldquo;Add data connection\u0026rdquo;. Here are the information you need to enter:\nName:\npipelines Access Key - REPLACE WITH YOUR USER ID:\nuserX Secret Key:\nminio123 Endpoint:\nhttps://minio-s3-minio.apps.riviera-dev-2024.sandbox2830.opentlc.com Region:\nnone Bucket - REPLACE WITH YOUR USER ID:\nuserX IMPORTANT: Once again, the bucket you will use has to match with the user ID you were provided\nThe result should look like: Create a Pipeline Server It is highly recommended to create your pipeline server before creating a workbench. So let\u0026rsquo;s do that now!\nOn the top menu click on \u0026ldquo;Pipelines\u0026rdquo;. Then click on \u0026ldquo;Configure pipeline server\u0026rdquo; Select the Data Connection created earlier (pipelines) and click the Configure pipeline server button: Wait for the pipeline server to finish its creation. When your pipeline server is ready, your screen will look like the following: At this point, your pipeline server is ready and deployed.\nIMPORTANT: You need to wait until that screen is ready. If it\u0026rsquo;s still spinning, wait for it to complete. If you continue and create your workbench before the pipeline server is ready, your workbench will not be able to submit pipelines to it.\n"
},
{
	"uri": "http://localhost:1313/development/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "1. Introduction The aim of this part is to familiarise you with the different microservices in the Crazy Train application. You will modify the code of several Quarkus, Python and Nodejs projects, understand how they interact and test the application as a whole.\nThe image below describes the overall operation\nBelow is a description of each service:\nCapture and process image (projectt capture-app): This is a Quarkus service that allows you to start, test and stop video capture. It exposes RESTful endpoints that can be called by other services or clients to control video capture.\nPredict command (project intelligent-train): This service is responsible for analysing the data from the captured video. It uses machine learning techniques to interpret the video data and provide information based on this analysis.\nPost process image (project train-ceq-app): is an event management service that receives information from Intelligent Train and other services, and triggers appropriate actions. For example, if the Intelligent Train detects an obstacle on the track, the service triggers an event to stop the train.\nControl Train (project train-controler): As its name suggests, this service is responsible for controlling the train itself. It would receive commands from services such as train-ceq and perform actions on the train, such as starting, stopping, changing speed, etc.\nMonitoring (project monitoring-app): This service is responsible for monitoring the entire system. It would collect data from all the other services, such as events triggered, actions performed, train status, etc., and provide an overview of the state of the system. It could also provide alerts or notifications in the event of problems being detected.\nEach service is independent and communicates with the others asynchronously (MQTT/Kafka). This allows great flexibility and scalability, as each service can be developed, deployed and scaled independently of the others.\n2. Your lab environment You are going to use OpenShift Dev Spaces. OpenShift Dev Spaces uses Kubernetes and containers to provide a consistent, secure, and zero-configuration development environment, accessible from a browser window.\nUse the following link to generate your Openshift Dev Space environment :\nLogin in with your OpenShift credentials (userX/yourpassword). If this is the first time you access Dev Spaces, you have to authorize Dev Spaces to access your account. In the Authorize Access window click on Allow selected permissions.\nThis opens the workspace, which will look pretty familiar if you are used to work with VS Code. Before opening the workspace, a pop-up might appear asking if you trust the contents of the workspace. Click Yes, I trust the authors to continue.\nThe workspace contains all the resources you are going to use during the workshop. In the project explorer on the left of the workspace, navigate to the rivieradev-app folder and look at the different projects.\n"
},
{
	"uri": "http://localhost:1313/development/capture-app/",
	"title": "Capture and pre process image",
	"tags": [],
	"description": "",
	"content": "capture-app is built using Quarkus, a full Java framework native to Kubernetes, designed for Java Virtual Machines (JVMs) and native compilation, optimising Java specifically for containers and enabling it to become an effective platform for serverless, cloud and Kubernetes environments.\nThe main functionality of the capture-app microservice is to control video capture. It offers the ability to start and stop video streaming, via exposed RESTful endpoints. These endpoints can be called from any http client (such as a web browser or a curl command in a terminal).\nIn the capture-app project, you will add two new properties to the application.properties file and modify the ScheduledCapture.java class to load these properties.\nModify the configuration file: Open the configuration file for your application. This is the file named src/main/resources/application.properties. Add the following properties: Add the two new properties: %dev.capture.mock=true %dev.capture.videoPath=/projects/rivieradev-app/capture-app/src/main/resources/videos/track-christmas-tree.avi Save your changes.\nOpen the file src/main/java/org/redhat/demo/crazytrain/captureimage/ScheduledCapture.java.\nAdd the @ConfigProperty annotations to load the new properties. Add these lines to the top of the class, just below the class declaration:\n@ConfigProperty(name = \u0026#34;capture.mock\u0026#34;) boolean mock; @ConfigProperty(name = \u0026#34;capture.videoPath\u0026#34;) String videoPath; Checking the code The src/main/java/com/train/capture/app/ScheduledCapture.java class should look like this:\npackage org.redhat.demo.crazytrain.captureimage; import jakarta.enterprise.context.ApplicationScoped; import jakarta.enterprise.event.Observes; import jakarta.inject.Inject; import jakarta.ws.rs.GET; import jakarta.ws.rs.POST; import jakarta.ws.rs.Path; import jakarta.ws.rs.core.Response; import jakarta.ws.rs.core.Response.ResponseBuilder; import java.util.ArrayList; import java.util.List; import org.eclipse.microprofile.config.inject.ConfigProperty; import org.eclipse.paho.client.mqttv3.MqttException; import org.jboss.logging.Logger; import org.opencv.core.Mat; import org.opencv.core.Size; import org.opencv.imgproc.Imgproc; import org.opencv.videoio.VideoCapture; import org.opencv.videoio.Videoio; import org.redhat.demo.crazytrain.mqtt.MqttPublisher; import org.redhat.demo.crazytrain.util.Util; import io.quarkus.runtime.StartupEvent; import io.quarkus.scheduler.Scheduled; import io.vertx.mutiny.core.Vertx; /** * ScheduledCapture is a service that captures images from a camera using the OpenCV library */ @ApplicationScoped @Path(\u0026#34;/capture\u0026#34;) public class ScheduledCapture{ private VideoCapture camera; /* add mock config property here */ /* add videoPath config property here */ // interval in milliseconds @ConfigProperty(name = \u0026#34;capture.interval\u0026#34;) int interval; // tmpFolder is the folder where the images are saved @ConfigProperty(name = \u0026#34;capture.tmpFolder\u0026#34;) String tmpFolder; // broker is the MQTT broker @ConfigProperty(name = \u0026#34;capture.brokerMqtt\u0026#34;) String broker; // topic is the MQTT topic @ConfigProperty(name = \u0026#34;capture.topic\u0026#34;) String topic; // nbImgSec is the number of images captured every second @ConfigProperty(name = \u0026#34;capture.periodicCapture\u0026#34;) int periodicCapture; @ConfigProperty(name = \u0026#34;capture.saveImage\u0026#34;) boolean saveImage; @ConfigProperty(name = \u0026#34;capture.videoDeviceIndex\u0026#34;) int videoDeviceIndex; @ConfigProperty(name = \u0026#34;capture.videoPeriodicCapture\u0026#34;) int videoPeriodicCapture; @Inject ImageCaptureService imageCaptureService; @Inject ImageService imageService; @Inject Vertx vertx; MqttPublisher mqttPublisher = null; private Long timerId; private volatile boolean stopRequested = false; private Thread testThread; private static final Logger LOGGER = Logger.getLogger(ScheduledCapture.class); Util util = null; public boolean isStopRequested() { return stopRequested; } public void setStopRequested(boolean stopRequested) { this.stopRequested = stopRequested; } // Start the camera when the application starts and set the resolution void onStart(@Observes StartupEvent ev) { Logger.getLogger(ScheduledCapture.class).info(\u0026#34;The application is starting...\u0026#34;); if(!mock){ camera = new VideoCapture(videoDeviceIndex); camera.set(Videoio.CAP_PROP_FRAME_WIDTH, 640); // Max resolution for Logitech C505 camera.set(Videoio.CAP_PROP_FRAME_HEIGHT, 480); // Max resolution for Logitech C505 camera.set(Videoio.CAP_PROP_EXPOSURE, 15); // Try to set exposure } util = new Util(); mqttPublisher = new MqttPublisher(broker.trim(), topic.trim()); } void readVideo(String videoPath) { VideoCapture capture = new VideoCapture(videoPath); if (!capture.isOpened()) { throw new IllegalArgumentException(\u0026#34;Video file not found at \u0026#34; + videoPath); } double fps = capture.get(Videoio.CAP_PROP_FPS); int frameSkip = (int) (fps/8); int count = 0; Mat frame = new Mat(); while (!stopRequested) { // Continue reading the video until a stop request is received while (capture.read(frame)) { if (count % frameSkip == 0) { // Publish the image to the MQTT broker long timestamp = System.currentTimeMillis(); if(util != null) { long start2 = System.nanoTime(); String jsonMessage = util.matToJson(frame, timestamp); long end2 = System.nanoTime(); LOGGER.debugf(\u0026#34;Time to convert image to json: %d ms\u0026#34;, (end2 - start2) / 1000000); LOGGER.debugf(\u0026#34;JSON Message with id %s\u0026#34;, jsonMessage); try { long start3 = System.nanoTime(); mqttPublisher.publish(jsonMessage); long end3 = System.nanoTime(); LOGGER.debugf(\u0026#34;Time to publish image: %d ms\u0026#34;, (end3 - start3) / 1000000); LOGGER.debugf(\u0026#34;Message with id %s published to topic: %s\u0026#34;, timestamp, topic); } catch (MqttException e) { e.printStackTrace(); } } if(saveImage){ String filepath = tmpFolder+\u0026#34;/\u0026#34; + timestamp + \u0026#34;.jpg\u0026#34;; imageService.saveImageAsync(frame, filepath).thenAccept(success -\u0026gt; { if (success) { LOGGER.debug(\u0026#34;Frame saved successfully\u0026#34;); } else { LOGGER.error(\u0026#34;Failed to save frame\u0026#34;); } }); } } count++; if (stopRequested) { // Check if stop has been requested inside the inner loop as well break; } } capture.set(Videoio.CAP_PROP_POS_FRAMES, 0); // Reset the video to the first frame } capture.release(); } // Capture and save a defined number of images every second void captureAndSaveImage() { LOGGER.debugf(\u0026#34;The Thread name is %s\u0026#34; + Thread.currentThread().getName()); // Capture the image long start = System.nanoTime(); Mat image = imageCaptureService.captureImage(this.camera); long end = System.nanoTime(); LOGGER.debugf(\u0026#34;Time to capture image: %d ms\u0026#34;, (end - start) / 1000000); // Publish the image to the MQTT broker long timestamp = System.currentTimeMillis(); if(util != null) { long start2 = System.nanoTime(); String jsonMessage = util.matToJson(image, timestamp); long end2 = System.nanoTime(); LOGGER.debugf(\u0026#34;Time to convert image to json: %d ms\u0026#34;, (end2 - start2) / 1000000); LOGGER.debugf(\u0026#34;JSON Message with id %s\u0026#34;, jsonMessage); try { long start3 = System.nanoTime(); mqttPublisher.publish(jsonMessage); // Check if stop has been requested if (stopRequested) { // Stop capture and release camera vertx.cancelTimer(timerId); timerId = null; imageCaptureService.releaseCamera(this.camera); mqttPublisher.disconnect(); LOGGER.info(\u0026#34;Capture stopped\u0026#34;); return; } long end3 = System.nanoTime(); LOGGER.debugf(\u0026#34;Time to publish image: %d ms\u0026#34;, (end3 - start3) / 1000000); LOGGER.debugf(\u0026#34;Message with id %s published to topic: %s\u0026#34;, timestamp, topic); } catch (MqttException e) { e.printStackTrace(); } } // Save the image to the file system (asynchronously) if(saveImage){ String filepath = tmpFolder+\u0026#34;/\u0026#34; + timestamp + \u0026#34;.jpg\u0026#34;; imageService.saveImageAsync(image, filepath).thenAccept(success -\u0026gt; { if (success) { LOGGER.debug(\u0026#34;Image saved successfully\u0026#34;); } else { LOGGER.error(\u0026#34;Failed to save image\u0026#34;); } }); } } @POST @Path(\u0026#34;/start\u0026#34;) public Response start() { LOGGER.info(\u0026#34;Capture started\u0026#34;); stopRequested = false; mqttPublisher.connect(); //captureEnabled = true; if (timerId != null) { return Response.status(Response.Status.BAD_REQUEST).entity(\u0026#34;Capture is already running\u0026#34;).build(); } timerId = vertx.setPeriodic(periodicCapture, id -\u0026gt; captureAndSaveImage()); return Response.ok(\u0026#34;Capture started\u0026#34;).build(); } @POST @Path(\u0026#34;/test\u0026#34;) public Response test() { LOGGER.info(\u0026#34;Test started\u0026#34;); stopRequested = false; mqttPublisher.connect(); if (timerId != null) { return Response.status(Response.Status.BAD_REQUEST).entity(\u0026#34;Capture is already running\u0026#34;).build(); } testThread = new Thread(() -\u0026gt; readVideo(videoPath)); testThread.start(); return Response.ok(\u0026#34;read video from file started\u0026#34;).build(); } @POST @Path(\u0026#34;/stop\u0026#34;) public Response stop() { stopRequested = true; LOGGER.info(\u0026#34;Stop requested\u0026#34;); if (testThread != null) { try { testThread.join(); // Wait for the testThread to finish testThread = null; } catch (InterruptedException e) { Thread.currentThread().interrupt(); // Restore interrupted status } } return Response.ok(\u0026#34;Stop requested\u0026#34;).build(); } } The application.properties file should look like this:\n%dev.quarkus.http.port=8082 %dev.capture.mock=true %dev.catpure.videoPath=/projects/rivieradev-app/capture-app/src/main/resources/videos/track-christmas-tree.avi %dev.catpure.videoPeriodicCapture=30 quarkus.kafka.devservices.enabled=false quarkus.swagger-ui.always-include=true capture.videoDeviceIndex=${VIDE0_DEVICE_INDEX:0} capture.dropbox.token=${DROPBOX_TOKEN:null} capture.tmpFolder=${TMP_FOLDER:/Users/mouchan/crazy-train-images} capture.interval=${INTERVAL:100} capture.periodicCapture=${PERIODIC_CAPTURE:30} capture.brokerMqtt=${MQTT_BROKER:tcp://localhost:1883} capture.topic=${MQTT_TOPIC:train-image} capture.videoPath=${VIDEO_PATH:/projects/rivieradev-app/capture-app/src/main/resources/videos/track-christmas-tree.avi} capture.videoPeriodicCapture=${VIDEO_PERIODIC_CAPTURE:30} capture.saveImage=${SAVE_IMAGE:false} capture.mock=${MOCK:false} quarkus.log.level=${LOGGER_LEVEL:INFO} Compiling the project Before committing your changes, you need to build the project to ensure that there are no compilation errors.\nOpen a terminal Run the commands below cd capture-app mvn clean package Check that there are no errors then close the terminal.\n"
},
{
	"uri": "http://localhost:1313/ai/creating-workbench/",
	"title": "Creating a workbench",
	"tags": [],
	"description": "",
	"content": "Launch a Workbench Once the Data Connection and Pipeline Server are fully created\nCreate a workbench Make sure it has the following characteristics:\nChoose a name for it, like: My Workbench Image Selection: CUSTOM Crazy train lab Container Size: Small Keep the default cluster storage settings On the bottom, tick \u0026ldquo;Use a data connection\u0026rdquo; Scroll down to \u0026ldquo;Use existing data connection\u0026rdquo; Select from the list the \u0026ldquo;pipelines\u0026rdquo; data connection you previously created. That should look like: Create the workbench and wait for your workbench to be fully started\nOnce it is, click the Open Link to connect to it. Authenticate with the same credentials as earlier\nYou will be asked to accept the following settings: Do so\nYou should now see this: Git-Clone the lab repo We will clone the content of our Git repo so that you can access all the materials that were created as part of our prototyping exercise.\nUsing the Git UI: Open the Git UI in Jupyter: Enter the URL of the Git repo: https://github.com/Demo-AI-Edge-Crazy-Train/workshop-model-training At this point, your project is ready for the work we want to do in it.\n"
},
{
	"uri": "http://localhost:1313/development/",
	"title": "Development (30min)",
	"tags": [],
	"description": "",
	"content": "Part 2 Development "
},
{
	"uri": "http://localhost:1313/overview/organization/",
	"title": "Organization",
	"tags": [],
	"description": "",
	"content": "TODO\n"
},
{
	"uri": "http://localhost:1313/devops/",
	"title": "DevOps (30min)",
	"tags": [],
	"description": "",
	"content": "Part 4 DevOps "
},
{
	"uri": "http://localhost:1313/overview/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "",
	"content": "OpenShift cluster TODO\nOpenShift cluster details OCP Cluster console URL : https://console-openshift-console.apps.TODO\nOCP Cluster API URL : https://api.TODO:6443\nThere is a dedicated OpenShift user for each utilisateur. On your table you will find a poster with the relevant information.\nTo connect to your Openshift cluster, click on the OCP Cluster console URL above and fill in your username and password. You will have acces to the Web Terminal by clicking on the \u0026gt;_ icon on the top right. The Web Terminal provides the oc client.\n"
},
{
	"uri": "http://localhost:1313/development/intelligent-train/",
	"title": "Predict command",
	"tags": [],
	"description": "",
	"content": "In this section we will import the new model trained before.\nOpen a new terminal Run the commands below, replace user_id with your assigned user name: cd intelligent-train curl -o models/model.onnx http://minio.minio:9000/\u0026lt;user_id\u0026gt;/models/model.onnx If you have not finish the training of the model use the commands below,\ncd intelligent-train curl -o models/model.onnx http://minio.minio:9000/model-registry/models/model.onnx "
},
{
	"uri": "http://localhost:1313/ai/retrain-model/",
	"title": "Retrain the model",
	"tags": [],
	"description": "",
	"content": "In this section you will navigate through the python code used to retrain the model. You will then adapt a data science pipeline and run it on Openshift. You will finally visualize your pipeline in Openshift AI dashboard and retreive it\u0026rsquo;s output in different formats.\nWARNING: You will run only the first steps of the model training inside the Jupyter Notebooks. The full traning will happen on Openshift side in order to limit the RAM needed for each participant. Running the model traning (transfer-learning.ipynb) notebook will crash due to OOM Killed. Your pod on Openshift will be deleted and automatically recreated. Nothing really bad but your environement will be down for a minute.\nNavigate through the code You previoulsy cloned a git repository. You should see on the file browser on the left hand side a folder that as the same name as the git projet: workshop-model-traning. Click on it. From there you should see several objects:\nThe utils/ folder contains helpers and dependencies for the model training such as python functions or mappers The inference/ folder contains material to query the models after you deployed them. We will use the content later on the lab. The traffic-signs.pipeline is a data science pipeline generated with Elyra. Elyra provides a graphical user interface that enables you to drag and drop your notebooks or python scripts and bind them together to create steps. You can run this pipeline in Openshift from the GUI. The labeling-extraction.ipynb notebook gets the images tagged with label studio. It downloads the images as well as their corresponding objects labeled with bounding boxes. The synthetic-data.ipynb notebook generates random synthetic data. Those are artifically created data that will add more data to the model training. The transfer-learning.ipynb notebook is the model training itself. The comparaison.ipynb notebook will compare the base model (that does not recognize the lego traffic signs) with the one you will train (that hopefully does). We want to ensure no regression on the model retraining. Extract the images and their annotations Click on labeling-extraction.ipynb. Run the whole notebook using the icon on the top: You may have notice that those scripts has created in your filesystem a dataset directory. This dataset directory contains a labels and a images sub-directory. It contains the extracted images and labels. Jump down on the same notebook on the section \u0026ldquo;Select a random image and display its boundind boxes\u0026rdquo;. Re-run that cell. It choose a random image from the dataset/images and display in the notebook the corresponding bounding boxes saved under dataset/labels.\nGenerate synthetic data You can close the preivous notebook and open the synthetic-data.ipynb notebook. This notebook generates random synthetic data. Those are artifically created data that will add more data to the model training. Run the whole notebook as explained on previous section. Have a quick look at the code and see exemples on the visualization sections. Re-run the visualizaton step to display more synthetic data exemples.\nReview the model traning step Be careful not to run the following notebook. It will crash your environement as we limited the allowed consommable RAM per environement.\nOpen transfer-learning.ipynb notebook and simply look at the code.\nReview the comparaison step Be careful not to run the following notebook. It will crash your environement as we limited the allowed consommable RAM per environement.\nOpen comparaison.ipynb notebook and simply look at the code.\nAdapt the data science pipeline You will now adpat a data science pipeline to make your training be scheduled on a GPU. There are few small shared GPUs where the traning will be executed. It should takes around 5minute for the training and comparaison part and arount 8min for the whole pipeline.\nOpen the traffic-signs.pipeline data science pipeline. Here you see a graphical interface where you can create and run your data science pipelines. The pipeline has been created by drag and dropping the steps in the UI.\nFix the pipeline You can notice that this pipeline has 4 nodes and 2 bindings. 1 binding is missing between the third (transfer-learning.ipynb) and fourth step (comparaison.ipynb). Click on the black dot on the right hand side of the third step (transfer-learning.ipynb). Hold down the mouse key until you reached the black dot of the left hand side of the fourth step (comparaison.ipynb).\nYou should have a similar result at the end: Review the node properties Right click on the second step of the pipeline (synthetic-data.ipynb). This will open a menu. Click on \u0026ldquo;Open Properties\u0026rdquo;. They appear on the right hand side. Scroll down and see some properties such as:\nRuntime Image: Is the container image that will be used to run the python code extracted from your notebooks. CPU request: Is the amount of CPU that should be available on the node for this particular step. RAM limit: Is the maximum amount of RAM allowed for this particular step (container will be killed otherwise). Pipeline Parameters: Make the globaly declared pipeline parameters available for this particular step. File Dependencies: Files that should be available on the container for the step execution. Here we need the whole utils/ directory. Output Files: Thoses files generated during execution will be available to subsequent pipeline steps. Kubernetes Secret: Mount a secret inside inside the container. Here we make the object storage credentials available as environement variable during the execution. You can notice on the top of the right hand side properties menu that 3 different panels are available (pipeline properties, pipeline parameters, node properties). Feel free to navigate to the other panels.\nRequest a GPU for the training step Right click on the third step of the pipeline (transfer-learning.ipynb). This will open a menu. Click on \u0026ldquo;Open Properties\u0026rdquo;. They appear on the right hand side. Look for the GPU property and select 1. This will request 1 GPU for your model traning. Scroll down to bottom.\nThe nodes containing GPUs has \u0026ldquo;Taints\u0026rdquo;. It means that by default no workload can be scheduled on nodes with taints. We need to add a toleration to allow the training step to use a GPU. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes.\nClick Add under Kubernetes Tolerations (at the bottom of the node properties panel). Fill up as follow:\nKey: type nvidia.com/gpu Operator: select Exists Effect: select NoSchedule You should have at the end: Run the pipeline It\u0026rsquo;s now time to run the pipeline on Openshift. Click on the \u0026ldquo;Run Pipeline\u0026rdquo; button on the top of Elyra GUI. See bellow: Click on the \u0026ldquo;Save and Submit\u0026rdquo; button.\nFill up the configurations. Choose 10 epochs as a pipeline parameter. It corresponds to the total number of iterations of all the training data in one cycle for training the machine learning model. Not enough epochs will cause your model to be inefficient. Too much epochs will cause your model to overfit (and thus beeing inefficient on predicting new data): After a few moments, you will see a successful popup displayed. From this popup, you can click on \u0026ldquo;Run Details\u0026rdquo; to skip some instructions of the next session. Visualizing your pipelines Retreive the pipeline runs If you missed the shortcut from elyra popup, follow thoses steps to retreive your pipeline run. Otherwise pass to the next paragraph. You can now go back to Openshift AI dashboard: https://rhods-dashboard-redhat-ods-applications.apps.riviera-dev-2024.sandbox2830.opentlc.com On the left hand side click on \u0026ldquo;Data Science Pipelines\u0026rdquo;, then on \u0026ldquo;Runs\u0026rdquo;. Here can see the pipeline execution. One run should be visible as you created one during the previous section. Click on it.\nYou can see the status of your pipeline run. If you click on a node it displays some information such as the \u0026ldquo;Logs\u0026rdquo; panel. If you select the \u0026ldquo;Main\u0026rdquo; container from that panel, you will see the logs associated to the notebooks execution: Wait for the pipeline to complete. You should have something like that: OPTIONAL - Retreiving pipelines output All the pipeline outputs are stored into the object storage. Connect to the S3 console using this link: https://minio-console-minio.apps.riviera-dev-2024.sandbox2830.opentlc.com. Connect with the same username we gave you at the begining of the lab. The password is minio123. You should see few buckets. Click on the one corresponding to your username. There should be one directory in the bucket that coresponds to your pipeline run. It starts with \u0026ldquo;traffic-sign\u0026rdquo;. Open it. Here you can see html, ipynb files and archives. Click on the comparaison.html file. A menu pops up on the right hand side. Click download and open this file locally on your browser. Note the diffenrece in the scores of the base and new model. In this exemple we lost precision on the original dataset. But we can now detect \u0026ldquo;lego\u0026rdquo; traffic signs with the new model. "
},
{
	"uri": "http://localhost:1313/development/train-ceq-app/",
	"title": "Post process image",
	"tags": [],
	"description": "",
	"content": "train-ceq-app is an application based on Apache Camel, a Java library for implementing application integrations using Enterprise Integration Patterns (EIP). This application is mainly composed of Camel routes defined in the PostProcessingRoute.xml file. These routes define how messages are consumed, transformed and forwarded to other services or destinations.\nThe postproscessing-route performs the following operations:\nMessage consumption: The route first consumes messages from the MQTT broker using the paho:{{train.mqtt.source.topicName}}?brokerUrl={{train.mqtt.brokerUrl}} URI. The messages consumed are then recorded in the log.\nSaving the initial message: The initial message is saved in the message header under the name \u0026ldquo;origin\u0026rdquo; so that it can be retrieved later.\nExtract Message ID: The message ID is extracted using the JSONPath expression $.id and stored in the message header.\nMessage deserialization: The message is deserialized into a Java object of type org.redhat.demo.crazytrain.model.Result using the Jackson library.\nMessage processing: The message is then processed by the CommandProcessor.\nPublish the message: The processed message is published on another MQTT topic using the URI paho:{{command.mqtt.destination.topicName}}?brokerUrl={{train.mqtt.brokerUrl}}.\nRetrieving the initial message: The initial message saved in the header is retrieved and put back in the message body.\nCloud Event Generation: A cloud event is generated from the initial message and processed by the CloudEventProcessor.\nCloud Event Publication: The cloud event is published on a Kafka topic using the URI kafka:{{monitoring.kafka.destination.topicName}}?brokers={{train.kafka.brokerUrl}}.\nThe command-capture-image route works in a similar way, but consumes messages from a Kafka topic, extracts a command from the message, and sends an HTTP POST request to a specified URL with the command as a parameter.\nIn a camel route the data is continuously transformed by various actions. Sometimes it is necessary to perform a check on the original message and not on the transformed message. It is good practice to save the original message so that it can be retrieved later. With Camel this is done via the properties of the message header.\nIn the train-ceq-app project, you are going to modify the PostProcessingRoute.xml file to save the initial message so that you can retrieve it later.\nOpen the file src/main/resources/camel/PostProcessingRoute.xml. Add the following instruction at the line \u0026lt;!-- add setHeader here --\u0026gt; : \u0026lt;setHeader name=\u0026#34;origin\u0026#34;\u0026gt;\u0026lt;simple\u0026gt;${body}\u0026lt;/simple\u0026gt;\u0026lt;/setHeader\u0026gt; This instruction saves the initial message in the message header under the name \u0026ldquo;origin\u0026rdquo;.\nNext, add the following statement at the line \u0026lt;!-- add setBody here --\u0026gt; : \u0026lt;setBody\u0026gt;\u0026lt;simple\u0026gt;${header.origin}\u0026lt;/simple\u0026gt;\u0026lt;/setBody\u0026gt; This instruction retrieves the original message saved in the header and puts it back in the message body.\nSave your changes. Now the src/main/resources/PostProcessingRoute.xml route saves the original message and retrieves it later. This allows you to check the original message and not the transformed message.\nThis is what the route should look like after your changes:\n\u0026lt;routes xmlns=\u0026#34;http://camel.apache.org/schema/spring\u0026#34;\u0026gt; \u0026lt;route id=\u0026#34;postproscesing-route\u0026#34;\u0026gt; \u0026lt;from uri=\u0026#34;paho:{{train.mqtt.source.topicName}}?brokerUrl={{train.mqtt.brokerUrl}}\u0026#34;/\u0026gt; \u0026lt;log loggingLevel=\u0026#34;DEBUG\u0026#34; message=\u0026#34;MQTT message received:\u0026#34;/\u0026gt; \u0026lt;log loggingLevel=\u0026#34;DEBUG\u0026#34; message=\u0026#34;${body}\u0026#34;/\u0026gt; \u0026lt;setHeader name=\u0026#34;origin\u0026#34;\u0026gt;\u0026lt;simple\u0026gt;${body}\u0026lt;/simple\u0026gt;\u0026lt;/setHeader\u0026gt; \u0026lt;setHeader name=\u0026#34;id\u0026#34;\u0026gt;\u0026lt;jsonpath\u0026gt;$.id\u0026lt;/jsonpath\u0026gt;\u0026lt;/setHeader\u0026gt; \u0026lt;log message=\u0026#34;Id of the message: ${header.id}\u0026#34;/\u0026gt; \u0026lt;unmarshal\u0026gt; \u0026lt;json library=\u0026#34;Jackson\u0026#34; unmarshalType=\u0026#34;org.redhat.demo.crazytrain.model.Result\u0026#34;/\u0026gt;\u0026lt;/unmarshal\u0026gt; \u0026lt;log message=\u0026#34;unmarshalling done\u0026#34;/\u0026gt; \u0026lt;process ref=\u0026#34;CommandProcessor\u0026#34;/\u0026gt; \u0026lt;log message=\u0026#34;Train Command: ${body}\u0026#34;/\u0026gt; \u0026lt;toD uri=\u0026#34;paho:{{command.mqtt.destination.topicName}}?brokerUrl={{train.mqtt.brokerUrl}}\u0026#34;/\u0026gt; \u0026lt;setBody\u0026gt;\u0026lt;simple\u0026gt;${header.origin}\u0026lt;/simple\u0026gt;\u0026lt;/setBody\u0026gt; \u0026lt;convertBodyTo type=\u0026#34;java.lang.String\u0026#34;/\u0026gt; \u0026lt;log loggingLevel=\u0026#34;DEBUG\u0026#34; message=\u0026#34;generating cloud event ${body}\u0026#34;/\u0026gt; \u0026lt;process ref=\u0026#34;CloudEventProcessor\u0026#34;/\u0026gt; \u0026lt;log loggingLevel=\u0026#34;DEBUG\u0026#34; message=\u0026#34;${body}\u0026#34;/\u0026gt; \u0026lt;toD uri=\u0026#34;kafka:{{monitoring.kafka.destination.topicName}}?brokers={{train.kafka.brokerUrl}}\u0026#34;/\u0026gt; \u0026lt;log loggingLevel=\u0026#34;DEBUG\u0026#34; message=\u0026#34;written into kafka\u0026#34;/\u0026gt; \u0026lt;/route\u0026gt; \u0026lt;route id=\u0026#34;command-capture-image\u0026#34;\u0026gt; \u0026lt;from uri=\u0026#34;kafka:{{monitoring.kafka.source.topicName}}?brokers={{train.kafka.brokerUrl}}\u0026#34;/\u0026gt; \u0026lt;setHeader name=\u0026#34;command\u0026#34;\u0026gt;\u0026lt;jsonpath\u0026gt;$.command\u0026lt;/jsonpath\u0026gt;\u0026lt;/setHeader\u0026gt; \u0026lt;toD uri=\u0026#34;{{train.http.url}}/${header.command}?httpMethod=POST\u0026#34; /\u0026gt; \u0026lt;/route\u0026gt; Compiling the project Before committing your changes, you need to build the project to ensure that there are no compilation errors.\nOpen a new terminal Run the commands below cd train-ceq-app mvn clean package Check that there are no errors then close the terminal.\n"
},
{
	"uri": "http://localhost:1313/ai/model-serving/",
	"title": "Model Serving",
	"tags": [],
	"description": "",
	"content": "At this point, you will deploy the model the just created into RHOAI model serving. If something went wrong with the model traning you can still do this section. Just follow the first \u0026ldquo;Fallback\u0026rdquo; section.\nOnce again, in the following objects that you will create, please change \u0026ldquo;userX\u0026rdquo; with your real user ID.\nFallback - You can skip if you have a tranined model In your Data Science project, create a data connection that refers to the global model registry where we stored a pre tranined model. To do so, go to your data science project, scroll down and click \u0026ldquo;Data Connections\u0026rdquo; or click directly to the \u0026ldquo;Data Connections\u0026rdquo; tab on the top menu. Please refer to this section if you have difficulties to create a data connection. Here is the info you need to enter: Name: Model Registry Access Key: userX - Change with your USER ID Secret Key: minio123 Endpoint: https://minio-s3-minio.apps.riviera-dev-2024.sandbox2830.opentlc.com Region: none Bucket: model-registry Create a Model Server In your project create a model server. You can click here to go to see all your deployed models: Click Add model server Here is the info you need to enter:\nModel server name: Traffic Sign Detection Serving runtime: OpenVINO Model Server Number of model server replicas to deploy: 1 Model server size Small Accelerator None Model route unchecked Token authorization unchecked The result should look like: You can click on Add to create the model server.\nDeploy the Model In your project, under Models and model servers select Deploy model.\nClick Deploy model Here is the information you will need to enter. If are on the fallback track, please change the \u0026ldquo;Existing data connection - Name\u0026rdquo; with the name of the data connection you created (Model Registry):\nModel name: new Model server: Traffic Sign Detection Model server - Model framework: onnx-1 Existing data connection - Name: pipelines - FOR FALLBACK track: use Model Registry Existing data connection - Path: models/model.onnx The result should look like: Click on Deploy.\nIf the model is successfully deployed you will see its status as green after few seconds. We will now confirm that the model is indeed working by querying it!\nQuerying the served Model Once the model is served, we can use it as an endpoint that can be queried. We\u0026rsquo;ll send a request to it, and get a result. This applies to anyone working within our cluster. This could either be colleagues, or applications.\nFirst, we need to get the URL of the model server.\nTo do this, click on the Internal Service link under the Inference endpoint column.\nIn the popup, you will see a few URLs for our model server. Note or copy the RestUrl, which should be something like http://modelmesh-serving.{user}:8008\nWe will now use this URL to query the model. Go back to the your running workbench i.e the jupyter notebooks environment.\nIn your running workbench, navigate to the notebook inference/inference.ipynb. Execute the cells of the notebook, and ensure you understand what is happening. The first section queries the base model that has been deployed globally for everyone. The second section takes your RestUrl endpoint and queries the model that you have tranined and deploy. You should see that with the base model, only the speed limit traffic signs are recognized. After your model re-training you now have a model that can better detect lego traffic signs. Congratulations!\n"
},
{
	"uri": "http://localhost:1313/development/monitoring-app/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "The monitoring-app is an application which monitors the status and behaviour of the train and its associated components. This microservice is responsible for :\nData collection: The monitoring-appp application collects data from a kafka topic. This includes events produced by train-ceq-app.\nData analysis: Once the data has been collected, the monitoring-appp application adds the predictions calculated above to the original image.\nData visualisation: The monitoring-appp application provides a user interface for viewing train data in real time.\nIn the monitoring-app project, you will modify certain properties and the code with the following instructions:\nModify the configuration file: Open the configuration file for your application. This is the file named src/main/resources/application.properties. Add the following properties: mp.messaging.incoming.train-monitoring.connector=smallrye-kafka mp.messaging.incoming.train-monitoring.topic=${KAFKA_TOPIC_MONITORING_NAME:train-monitoring} mp.messaging.incoming.train-monitoring.cloud-events=false mp.messaging.incoming.train-monitoring.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer These properties configure the application to use the SmallRye Kafka connector to read messages from the Kafka train-monitoring topic. The deserializer is configured to convert the Kafka messages, which are bytes, into character strings.\n**Modify the ImageProcessing class: Open the file src/main/java/org/redhat/demo/crazytrain/processing/ImageProcessing.java. Add the @Incoming(\u0026quot;train-monitoring\u0026quot;) annotation to the process method. Below is the result:\nThe @Incoming annotation indicates that this method should be called whenever a message is read from the train-monitoring channel. The message is passed to the method as a parameter.\nThese modifications allow our application to consume messages from the Kafka train-monitoring topic and process them with the process method of the ImageProcessing class.\nChecking the code The src/main/java/org/redhat/demo/crazytrain/processing/ImageProcessing.java class should look like this:\npackage org.redhat.demo.crazytrain.processing; import java.util.Base64; import java.util.concurrent.TimeUnit; import org.eclipse.microprofile.reactive.messaging.Incoming; import jakarta.ws.rs.core.MediaType; import jakarta.ws.rs.GET; import jakarta.ws.rs.Path; import jakarta.ws.rs.Produces; import jakarta.annotation.PostConstruct; import jakarta.inject.Inject; import org.eclipse.microprofile.config.inject.ConfigProperty; import org.jboss.logging.Logger; import org.opencv.core.CvType; import org.opencv.core.Mat; import org.redhat.demo.crazytrain.services.SaveService; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.JsonMappingException; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import io.micrometer.core.instrument.MeterRegistry; import io.micrometer.core.instrument.Timer; import io.smallrye.mutiny.Multi; import io.smallrye.mutiny.operators.multi.processors.BroadcastProcessor; @Path(\u0026#34;/train-monitoring\u0026#34;) public class ImageProcessing { private static final Logger LOGGER = Logger.getLogger(ImageProcessing.class); private final BroadcastProcessor\u0026lt;String\u0026gt; broadcastProcessor = BroadcastProcessor.create(); @Inject SaveService saveService; @ConfigProperty(name = \u0026#34;monitoring.saveImage\u0026#34;) boolean saveImage; @ConfigProperty(name = \u0026#34;monitoring.tmpFolder\u0026#34;) String tmpFolder; @Inject MeterRegistry registry; Timer timer; @PostConstruct void init() { timer = Timer.builder(\u0026#34;image.processing.time\u0026#34;) .description(\u0026#34;Time taken to get a message from Kafka and process it\u0026#34;) .register(registry); } @Incoming(\u0026#34;train-monitoring\u0026#34;) public void process(String result) { LOGGER.debug(\u0026#34;Consumer kafka recived : \u0026#34;+result); long start = System.nanoTime(); ObjectMapper mapper = new ObjectMapper(); JsonNode jsonNode; try { jsonNode = mapper.readTree(result); JsonNode data = jsonNode.get(\u0026#34;data\u0026#34;); String imageBytesBase64 = data.get(\u0026#34;image\u0026#34;).asText(); broadcastProcessor.onNext(imageBytesBase64); long end = System.nanoTime(); timer.record(end - start, TimeUnit.NANOSECONDS); } catch (JsonMappingException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (JsonProcessingException e) { // TODO Auto-generated catch block e.printStackTrace(); } } @GET @Produces(MediaType.SERVER_SENT_EVENTS) public Multi\u0026lt;String\u0026gt; stream() { return broadcastProcessor.toHotStream(); } } The application.properties file should look like :\n%dev.quarkus.http.port=8086 # Configure the Kafka source kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS:localhost:9092} mp.messaging.incoming.train-monitoring.connector=smallrye-kafka mp.messaging.incoming.train-monitoring.topic=${KAFKA_TOPIC_MONITORING_NAME:train-monitoring} mp.messaging.incoming.train-monitoring.cloud-events=false mp.messaging.incoming.train-monitoring.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer mp.messaging.outgoing.commands-out.connector=smallrye-kafka mp.messaging.outgoing.commands-out.topic=${KAFKA_TOPIC_COMMAND_CAPTURE_NAME:train-command-capture} mp.messaging.outgoing.commands-out.value.serializer=org.apache.kafka.common.serialization.StringSerializer monitoring.saveImage=${SAVE_IMAGE:false} monitoring.tmpFolder=${TMP_FOLDER:/tmp/crazy-train-images} quarkus.log.level=${LOGGER_LEVEL:INFO} quarkus.swagger-ui.always-include=true %dev.kafka.topic.train-command-capture.replication.factor=1 %dev.kafka.topic.train-monitoring.replication.factor=1 Compiling the project Before committing your changes, you need to build the project to ensure that there are no compilation errors.\nOpen a new terminal Run the commands below cd monitoring-app mvn clean package Check that there are no errors then close the terminal.\n"
},
{
	"uri": "http://localhost:1313/development/start-services/",
	"title": "Start services",
	"tags": [],
	"description": "",
	"content": " From Devspaces, click on the search bar on the top and choose \u0026ldquo;Run Task\u0026rdquo; from the drop-down list. ! Run Task menu\nSelect the start-all-apps task, this task will run all the previously modified applications in parallel.\nClick on Continue without scanning the task outoput. Each application will start in a terminal. Terminals are accessible from the bottom right, Select \u0026rsquo;no\u0026rsquo; on the pop-up which indicates that a new process has started and that it is possible to make a port redirection.\nTo check that all the applications have started, you should have the following logs: Log capture-app intelligent-train log**! Log intelligent-train application](/images/dev-section/intelligent-train-log.png)\nLog train-ceq-app Train-ceq-app log (/images/dev-section/train-ceq-log.png)\nMonitoring-app log**! Log train-controller Now that all the applications have been started, we\u0026rsquo;re going to simulate the operation of our intelligent train!\n"
},
{
	"uri": "http://localhost:1313/development/test-services/",
	"title": "Test services",
	"tags": [],
	"description": "",
	"content": "We are going to simulate the operation of the train:\nOpen a new terminal Execute the command below to retrieve the URL of the monitoring console: oc get routes -o jsonpath=\u0026#39;{range .items[*]}{.metadata.annotations.che\\.routing\\.controller\\.devfile\\.io/endpoint-name}{\u0026#34;\\t\u0026#34;}{.spec.host}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; | grep monitoring-svc | cut -f 2 Copy the URL, launch a new browser window in anonymous mode (in order to have an empty cache), insert the URL. Return to your terminal and run the following command: curl -X \u0026#39;POST\u0026#39; \u0026#39;http://localhost:8082/capture/test\u0026#39; -H \u0026#39;accept: */*\u0026#39; From your browser, you should be able to see the train simulation and the traffic sign detection in real time. If not, refresh your browser. Well done! The simulation worked well :) now you can stop the simulation by closing all the terminals.\n"
},
{
	"uri": "http://localhost:1313/development/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations, you\u0026rsquo;ve finished the development section! You should now have a better understanding of the Crazy Train application and its microservices-based architecture. The lab isn\u0026rsquo;t over yet! In the next section, we\u0026rsquo;re going to find out how to deploy our applications using gitops tools.\n"
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/",
	"title": "Welcome",
	"tags": [],
	"description": "",
	"content": "Riviera Dev 2024 - Lab \u0026ldquo;Crazy Train\u0026rdquo; Welcome, dear Red Hatter, in this Lab where you will discover a part of the Red Hat offering related to Edge Computing and AI.\nYou will play with the following technologies:\nTODO In this lab, you will TODO\nFun are expected ahead!\n"
}]